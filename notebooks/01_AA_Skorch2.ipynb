{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "pathRuns = cfg['filePaths']['dirProcessedFiles']['dirRuns']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ../cluster/data/medinfmk/ddi/processed/runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data(input_fea, input_lab, seperate=False):\n",
    "#     offside_sim_path = input_fea\n",
    "#     drug_interaction_matrix_path = input_lab\n",
    "#     drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "#     interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "#     #print(drug_fea.shape)\n",
    "#     #print(interaction.shape)\n",
    "#     #return\n",
    "#     train = []\n",
    "#     label = []\n",
    "#     tmp_fea=[]\n",
    "#     drug_fea_tmp = []\n",
    "#     for i in range(0, interaction.shape[0]):\n",
    "#         for j in range(0, interaction.shape[1]):\n",
    "#             label.append(interaction[i,j])\n",
    "#             drug_fea_tmp = list(drug_fea[i])\n",
    "#             if seperate:\n",
    "        \n",
    "#                  tmp_fea = (drug_fea_tmp,drug_fea_tmp)\n",
    "\n",
    "#             else:\n",
    "#                  tmp_fea = drug_fea_tmp + drug_fea_tmp\n",
    "#             train.append(tmp_fea)\n",
    "\n",
    "#     return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    #print(drug_fea.shape)\n",
    "    #print(interaction.shape)\n",
    "    #return\n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "            \n",
    "    for i in range(0, (interaction.shape[0]-1)):\n",
    "        for j in range((i+1), interaction.shape[1]):\n",
    "            #print(i,j)\n",
    "    #return\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp_1 = list(drug_fea[i])\n",
    "            drug_fea_tmp_2 = list(drug_fea[j])\n",
    "            if seperate:\n",
    "                 tmp_fea = (drug_fea_tmp_1,drug_fea_tmp_2)\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp_1 + drug_fea_tmp_2\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "        print(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_prep = np.repeat(np.arange(1,6),5).reshape((-1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_prep = np.random.binomial(1, 0.5, size = 25).reshape((5,5))\n",
    "#y_prep = np.arange(0,25).reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fea = pathInput+DS1_path+\"/offsideeffect_Jacarrd_sim.csv\"\n",
    "###input_fea = pathInput+DS1_path+\"/dummy/X_dummy.csv\"\n",
    "###input_fea = pathInput+DS1_path+\"/chem_Jacarrd_sim.csv\"\n",
    "###input_fea = pathOutput+\"/finalsimddd.txt\"\n",
    "input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "###input_lab = pathInput+DS1_path+\"/dummy/y_dummy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "#     return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(input_fea, X_prep.astype(int), fmt='%i', delimiter=\",\")\n",
    "# np.savetxt(input_lab, y_prep.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = prepare_data(input_fea, input_lab, seperate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149878, 1096)"
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149878,)"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_data1, X_data2 = transfer_array_format(X)\n",
    "#X = np.concatenate((X_data1, X_data2), axis = 1)\n",
    "###Y, encoder = preprocess_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPicklePath = pathPickles+\"/data_X_y_chem_Jaccard.p\"\n",
    "dataPicklePath = pathPickles+\"/data_X_y_offside_Jaccard.p\"\n",
    "#dataPicklePath = pathPickles+\"/data_X_y_SNFmat.p\"\n",
    "\n",
    "with open(dataPicklePath, 'wb') as f:\n",
    "    pickle.dump([X, y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dataPicklePath, 'rb') as f:\n",
    "#     X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X, y = make_classification(1500, 1000, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX = torch.from_numpy(X).type(torch.float32)\n",
    "# ty = torch.from_numpy(y).type(torch.int64)\n",
    "\n",
    "# dataSet = TensorDataset(tX, ty)\n",
    "# dataLoader = DataLoader(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_available_cuda_devices():\n",
    "#     n_gpu = torch.cuda.device_count()\n",
    "#     print('number of GPUs available:', n_gpu)\n",
    "#     for i in range(n_gpu):\n",
    "#         print(\"cuda:{}, name:{}\".format(i, torch.cuda.get_device_name(i)))\n",
    "#         device = torch.device('cuda', i)\n",
    "#         get_cuda_device_stats(device)\n",
    "#         print()\n",
    "        \n",
    "# def get_cuda_device_stats(device):\n",
    "#     print('total memory available:', torch.cuda.get_device_properties(device).total_memory/(1024**3), 'GB')\n",
    "#     print('total memory allocated on device:', torch.cuda.memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('max memory allocated on device:', torch.cuda.max_memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('total memory cached on device:', torch.cuda.memory_cached(device)/(1024**3), 'GB')\n",
    "#     print('max memory cached  on device:', torch.cuda.max_memory_cached(device)/(1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=model_input_dim, H1=400, H2=300, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model\n",
    "model_input_dim = X.shape[1]\n",
    "D_in, H1, H2, D_out, drop = model_input_dim, 400, 300, 2, 0.5\n",
    "# Training\n",
    "#batch_size, epochs = 100, 20\n",
    "#print_iter = int(epochs / 10)\n",
    "# SGD\n",
    "#learning_rate, momentum, weight_decay, nesterov = 0.01, 0.9, 1e-6, True\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = NDD(D_in, H1, H2, D_out, drop)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #device = \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "writer = SummaryWriter(pathRuns+\"test_40epochs_100batch_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc = EpochScoring(scoring='roc_auc', lower_is_better=False)\n",
    "#callbacks.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(TensorBoard(writer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer=SGD(momentum=0.9, weight_decay=1e-6, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=20,\n",
    "    optimizer=SGD,\n",
    "    optimizer__lr=0.01,\n",
    "    optimizer__momentum=0.9,    \n",
    "    optimizer__weight_decay=1e-6,    \n",
    "    optimizer__nesterov=True,    \n",
    "    batch_size=100,\n",
    "    callbacks=callbacks,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('net', net),\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X, y)\n",
    "# y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataLoader:\n",
    "#     X,y = data\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(\"Outside: input size\", X.size(), y.size(), X.device, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.1],\n",
    "#     'max_epochs': [5],\n",
    "#     'module__H1': [300],\n",
    "#     'module__H2': [200, 100],\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy')\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5503\u001b[0m       \u001b[32m0.6003\u001b[0m        \u001b[35m0.6463\u001b[0m  3.0106\n",
      "      2        \u001b[36m0.4986\u001b[0m       \u001b[32m0.7117\u001b[0m        \u001b[35m0.5407\u001b[0m  3.0374\n",
      "      3        \u001b[36m0.4904\u001b[0m       0.6668        0.5783  2.9714\n",
      "      4        \u001b[36m0.4861\u001b[0m       0.7091        0.5552  2.5985\n",
      "      5        \u001b[36m0.4861\u001b[0m       \u001b[32m0.7429\u001b[0m        \u001b[35m0.5060\u001b[0m  3.0570\n",
      "      6        \u001b[36m0.4804\u001b[0m       0.7058        0.5437  3.0792\n",
      "      7        0.4806       \u001b[32m0.7637\u001b[0m        \u001b[35m0.4919\u001b[0m  2.9267\n",
      "      8        0.4823       \u001b[32m0.7676\u001b[0m        \u001b[35m0.4918\u001b[0m  2.9078\n",
      "      9        \u001b[36m0.4783\u001b[0m       0.7648        0.4933  3.0386\n",
      "     10        \u001b[36m0.4775\u001b[0m       0.7605        0.4976  2.9310\n",
      "     11        \u001b[36m0.4766\u001b[0m       0.7578        0.4921  3.2826\n",
      "     12        \u001b[36m0.4664\u001b[0m       0.7622        \u001b[35m0.4890\u001b[0m  3.1057\n",
      "     13        \u001b[36m0.4660\u001b[0m       0.7595        0.4897  2.8943\n",
      "     14        \u001b[36m0.4626\u001b[0m       0.7518        0.4988  2.7844\n",
      "     15        0.4686       0.7132        0.5280  2.8499\n",
      "     16        0.4732       0.7559        0.5055  2.8966\n",
      "     17        0.4783       0.7264        0.5125  3.4756\n",
      "     18        0.4814       0.7151        0.5308  3.0031\n",
      "     19        0.4773       0.7202        0.5346  3.5369\n",
      "     20        0.4708       0.7066        0.5507  2.9365\n",
      "     21        0.4773       0.7573        0.5034  2.9735\n",
      "     22        0.4859       0.7449        0.5158  3.3379\n",
      "     23        0.4712       0.7484        0.5008  2.9296\n",
      "     24        0.4706       0.7422        0.5059  2.8426\n",
      "     25        0.4722       0.6997        0.5238  3.1344\n",
      "     26        0.4818       0.7349        0.5262  2.9035\n",
      "     27        0.4730       0.7070        0.5598  3.0044\n",
      "     28        \u001b[36m0.4614\u001b[0m       \u001b[32m0.7687\u001b[0m        0.5015  2.8937\n",
      "     29        0.4627       0.7313        0.5298  2.8944\n",
      "     30        \u001b[36m0.4612\u001b[0m       0.7004        0.5509  2.9109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=NDD(\n",
       "    (fc1): Linear(in_features=1096, out_features=400, bias=True)\n",
       "    (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
       "    (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 1328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6829205482702533,\n",
       " 0.6000645577792124,\n",
       " 0.4371237772761475,\n",
       " 0.9566694112803623)"
      ]
     },
     "execution_count": 1330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred), f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
